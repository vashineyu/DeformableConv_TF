{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow import nn\n",
    "\n",
    "class DeformableConvLayer(layers.Conv2D):\n",
    "    \"\"\"Only support \"channel last\" data format\"\"\"\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 num_deformable_group=None,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"`kernel_size`, `strides` and `dilation_rate` must have the same value in both axis.\n",
    "        :param num_deformable_group: split output channels into groups, offset shared in each group. If\n",
    "        this parameter is None, then set  num_deformable_group=filters.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.offset_layer_kernel = None\n",
    "        self.offset_layer_bias = None\n",
    "        if num_deformable_group is None:\n",
    "            num_deformable_group = filters\n",
    "        if filters % num_deformable_group != 0:\n",
    "            raise ValueError('\"filters\" mod \"num_deformable_group\" must be zero')\n",
    "        self.num_deformable_group = num_deformable_group\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        # kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "        # we want to use depth-wise conv\n",
    "        kernel_shape = self.kernel_size + (self.filters * input_dim, 1)\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=(self.filters,),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                trainable=True,\n",
    "                dtype=self.dtype)\n",
    "\n",
    "        # create offset conv layer\n",
    "        offset_num = self.kernel_size[0] * self.kernel_size[1] * self.num_deformable_group\n",
    "        #print(\"offset_num: %s\" % offset_num)\n",
    "        self.offset_layer_kernel = self.add_weight(\n",
    "            name='offset_layer_kernel',\n",
    "            shape=self.kernel_size + (input_dim, offset_num * 2),  # 2 means x and y axis\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        #print(self.offset_layer_kernel)\n",
    "        self.offset_layer_bias = self.add_weight(\n",
    "            name='offset_layer_bias',\n",
    "            shape=(offset_num * 2,),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            # initializer=tf.random_uniform_initializer(-5, 5),\n",
    "            regularizer=self.bias_regularizer,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        # get offset, shape [batch_size, out_h, out_w, filter_h, * filter_w * channel_out * 2]\n",
    "        offset = nn.conv2d(inputs,\n",
    "                          filter=self.offset_layer_kernel,\n",
    "                          strides=[1, *self.strides, 1],\n",
    "                          padding=self.padding.upper(),\n",
    "                          dilations=[1, *self.dilation_rate, 1])\n",
    "        # shape of offset: n_batch, feature_map_x, feature_map_y, offset_num*2\n",
    "        offset += self.offset_layer_bias\n",
    "\n",
    "        # add padding if needed\n",
    "        inputs = self._pad_input(inputs)\n",
    "\n",
    "        # some length\n",
    "        batch_size = inputs.get_shape().as_list()[0]\n",
    "        channel_in = inputs.get_shape().as_list()[-1]\n",
    "        in_h, in_w = inputs.get_shape().as_list()[1:3]  # input feature map size\n",
    "        out_h, out_w = offset.get_shape().as_list()[1:3]  # output feature map size\n",
    "        filter_h, filter_w = self.kernel_size\n",
    "\n",
    "        # get x, y axis offset\n",
    "        offset = tf.reshape(offset, [batch_size, out_h, out_w, -1, 2])\n",
    "        y_off, x_off = offset[:, :, :, :, 0], offset[:, :, :, :, 1]\n",
    "\n",
    "        # input feature map gird coordinates\n",
    "        y, x = self._get_conv_indices([in_h, in_w]) # [1, out_h, out_w, filter_h * filter_w]\n",
    "        y, x = [tf.expand_dims(i, axis=-1) for i in [y, x]]\n",
    "        y, x = [tf.tile(i, [batch_size, 1, 1, 1, self.num_deformable_group]) for i in [y, x]] # make batch_dim and filter_dim equal to kernel\n",
    "        y, x = [tf.reshape(i, [*i.shape[0: 3], -1]) for i in [y, x]]\n",
    "        y, x = [tf.to_float(i) for i in [y, x]]\n",
    "        # print(y, x)\n",
    "        # add offset\n",
    "        y, x = y + y_off, x + x_off\n",
    "        y = tf.clip_by_value(y, 0, in_h - 1)\n",
    "        x = tf.clip_by_value(x, 0, in_w - 1)\n",
    "\n",
    "        # get four coordinates of points around (x, y)\n",
    "        y0, x0 = [tf.to_int32(tf.floor(i)) for i in [y, x]]\n",
    "        y1, x1 = y0 + 1, x0 + 1\n",
    "        # clip\n",
    "        y0, y1 = [tf.clip_by_value(i, 0, in_h - 1) for i in [y0, y1]]\n",
    "        x0, x1 = [tf.clip_by_value(i, 0, in_w - 1) for i in [x0, x1]]\n",
    "\n",
    "        # get pixel values\n",
    "        indices = [[y0, x0], [y0, x1], [y1, x0], [y1, x1]]\n",
    "        p0, p1, p2, p3 = [self._get_pixel_values_at_point(inputs, i) for i in indices]\n",
    "        # cast to float\n",
    "        x0, x1, y0, y1 = [tf.to_float(i) for i in [x0, x1, y0, y1]]\n",
    "        # weights\n",
    "        w0 = (y1 - y) * (x1 - x)\n",
    "        w1 = (y1 - y) * (x - x0)\n",
    "        w2 = (y - y0) * (x1 - x)\n",
    "        w3 = (y - y0) * (x - x0)\n",
    "        # expand dim for broadcast\n",
    "        w0, w1, w2, w3 = [tf.expand_dims(i, axis=-1) for i in [w0, w1, w2, w3]]\n",
    "        # bilinear interpolation\n",
    "        pixels = tf.add_n([w0 * p0, w1 * p1, w2 * p2, w3 * p3])\n",
    "\n",
    "        # reshape the \"big\" feature map\n",
    "        pixels = tf.reshape(pixels, [batch_size, out_h, out_w, filter_h, filter_w, self.num_deformable_group, channel_in])\n",
    "        pixels = tf.transpose(pixels, [0, 1, 3, 2, 4, 5, 6])\n",
    "        pixels = tf.reshape(pixels, [batch_size, out_h * filter_h, out_w * filter_w, self.num_deformable_group, channel_in])\n",
    "\n",
    "        # copy channels to same group\n",
    "        feat_in_group = self.filters // self.num_deformable_group\n",
    "        pixels = tf.tile(pixels, [1, 1, 1, 1, feat_in_group])\n",
    "        pixels = tf.reshape(pixels, [batch_size, out_h * filter_h, out_w * filter_w, -1])\n",
    "\n",
    "        # depth-wise conv\n",
    "        out = tf.nn.depthwise_conv2d(pixels, self.kernel, [1, filter_h, filter_w, 1], 'VALID')\n",
    "        # add the output feature maps in the same group\n",
    "        out = tf.reshape(out, [batch_size, out_h, out_w, self.filters, channel_in])\n",
    "        out = tf.reduce_sum(out, axis=-1)\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return self.activation(out)\n",
    "\n",
    "    def _pad_input(self, inputs):\n",
    "        \"\"\"Check if input feature map needs padding, because we don't use the standard Conv() function.\n",
    "        :param inputs:\n",
    "        :return: padded input feature map\n",
    "        \"\"\"\n",
    "        # When padding is 'same', we should pad the feature map.\n",
    "        # if padding == 'same', output size should be `ceil(input / stride)`\n",
    "        if self.padding == 'same':\n",
    "            in_shape = inputs.get_shape().as_list()[1: 3]\n",
    "            padding_list = []\n",
    "            for i in range(2):\n",
    "                filter_size = self.kernel_size[i]\n",
    "                dilation = self.dilation_rate[i]\n",
    "                dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "                same_output = (in_shape[i] + self.strides[i] - 1) // self.strides[i]\n",
    "                valid_output = (in_shape[i] - dilated_filter_size + self.strides[i]) // self.strides[i]\n",
    "                if same_output == valid_output:\n",
    "                    padding_list += [0, 0]\n",
    "                else:\n",
    "                    p = dilated_filter_size - 1\n",
    "                    p_0 = p // 2\n",
    "                    padding_list += [p_0, p - p_0]\n",
    "            if sum(padding_list) != 0:\n",
    "                padding = [[0, 0],\n",
    "                           [padding_list[0], padding_list[1]],  # top, bottom padding\n",
    "                           [padding_list[2], padding_list[3]],  # left, right padding\n",
    "                           [0, 0]]\n",
    "                inputs = tf.pad(inputs, padding)\n",
    "        return inputs\n",
    "\n",
    "    def _get_conv_indices(self, feature_map_size):\n",
    "        \"\"\"the x, y coordinates in the window when a filter sliding on the feature map\n",
    "        :param feature_map_size:\n",
    "        :return: y, x with shape [1, out_h, out_w, filter_h * filter_w]\n",
    "        \"\"\"\n",
    "        feat_h, feat_w = [int(i) for i in feature_map_size[0: 2]]\n",
    "\n",
    "        x, y = tf.meshgrid(tf.range(feat_w), tf.range(feat_h)) # shape: 2d, x: h*w, y: h*w\n",
    "        # reshape meshgrid into 1,h,w,1\n",
    "        x, y = [tf.reshape(i, [1, *i.get_shape(), 1]) for i in [x, y]]  # shape: 4d, [1, h, w, 1]\n",
    "        x, y = [tf.image.extract_image_patches(i,\n",
    "                                               [1, *self.kernel_size, 1],\n",
    "                                               [1, *self.strides, 1],\n",
    "                                               [1, *self.dilation_rate, 1],\n",
    "                                               'VALID')\n",
    "                for i in [x, y]]  # shape [1, out_h, out_w, filter_h * filter_w]\n",
    "        return y, x\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_pixel_values_at_point(inputs, indices):\n",
    "        \"\"\"get pixel values\n",
    "        :param inputs:\n",
    "        :param indices: shape [batch_size, H, W, I], I = filter_h * filter_w * channel_out\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        y, x = indices\n",
    "        batch, h, w, n = y.get_shape().as_list()[0: 4]\n",
    "\n",
    "        batch_idx = tf.reshape(tf.range(0, batch), (batch, 1, 1, 1))\n",
    "        b = tf.tile(batch_idx, (1, h, w, n))\n",
    "        pixel_idx = tf.stack([b, y, x], axis=-1)\n",
    "        return tf.gather_nd(inputs, pixel_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# train.py / main\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import get_cfg_defaults\n",
    "from DeformableConv_TF.model import preproc_fn, build_model\n",
    "cfg = get_cfg_defaults()\n",
    "\n",
    "devices = \",\".join(str(i) for i in cfg.SYSTEM.DEVICES)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "\n",
    "from dataloader import GetDataset, DataLoader, TrainingAugmentation, TestingAugmentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "trainset, testset = load_data()\n",
    "x_train, y_train = trainset\n",
    "x_test, y_test = testset\n",
    "\n",
    "idx = np.arange(len(x_train))\n",
    "idx_train, idx_valid = train_test_split(idx, test_size=0.1)\n",
    "x_train, x_valid = x_train[idx_train], x_train[idx_valid]\n",
    "y_train, y_valid = y_train[idx_train], y_train[idx_valid]\n",
    "\n",
    "dataset_train = GetDataset(x=x_train, y=y_train, num_classes=10, \n",
    "                           preproc_fn=preproc_fn, augment_fn=TrainingAugmentation.augmentation)\n",
    "dataset_valid = GetDataset(x=x_valid, y=y_valid, num_classes=10, \n",
    "                           preproc_fn=preproc_fn, augment_fn=TestingAugmentation.augmentation)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset_train, batch_size=cfg.MODEL.BATCH_SIZE)\n",
    "\n",
    "x_valid, y_valid = next(iter(DataLoader(dataset_valid, batch_size=len(dataset_valid))))\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 57,034\n",
      "Trainable params: 56,714\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=x_valid.shape[1:], output_num=y_valid.shape[-1], use_deformable=False)\n",
    "optim = tf.keras.optimizers.SGD(lr=cfg.MODEL.LEARNING_RATE, nesterov=True, momentum=0.95)\n",
    "#optim = tf.keras.optimizers.Adam(lr=cfg.MODEL.LEARNING_RATE)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=optim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 48s 226ms/step - loss: 1.2966 - acc: 0.6277 - val_loss: 7.8468 - val_acc: 0.0977\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 46s 216ms/step - loss: 0.6470 - acc: 0.8226 - val_loss: 6.2652 - val_acc: 0.1082\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 45s 215ms/step - loss: 0.4817 - acc: 0.8630 - val_loss: 0.9646 - val_acc: 0.6898\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 43s 206ms/step - loss: 0.4042 - acc: 0.8845 - val_loss: 1.6566 - val_acc: 0.5177\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 45s 211ms/step - loss: 0.3545 - acc: 0.8974 - val_loss: 1.0101 - val_acc: 0.6722\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 45s 212ms/step - loss: 0.3156 - acc: 0.9088 - val_loss: 0.6888 - val_acc: 0.7762\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 44s 210ms/step - loss: 0.2915 - acc: 0.9150 - val_loss: 0.8456 - val_acc: 0.7133\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 44s 210ms/step - loss: 0.2721 - acc: 0.9192 - val_loss: 0.8110 - val_acc: 0.7385\n",
      "Epoch 9/100\n",
      "127/211 [=================>............] - ETA: 17s - loss: 0.2650 - acc: 0.9221"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-264b7c7d419c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_data=(x_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(dataloader, \n",
    "                    epochs=cfg.MODEL.EPOCHS, \n",
    "                    steps_per_epoch=len(dataloader), \n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
